{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "Q1: Derive the negative log-likelihood cost function J(x, y; Î¸)\n",
    "Consider the following regression network:\n",
    "\\begin{equation*}\n",
    "    f(x;w,b) = w^Tx+b\n",
    "\\end{equation*}\n",
    "And the model has a Gaussian distribution of prediction errors and with a unity covariance matrix $p(y_i;\\theta)$, whereas the mean is $f(x_i;\\theta)$ and variance is one.\\newline\n",
    "It is known that the negative log-likelihood cost function is:\n",
    "\\begin{equation*}\n",
    "    J(\\theta) = -log\\Pi^{m-1}_{i=0}\\hat{P}_{model}(y^{(i)}|x^{(i)},\\theta)\n",
    "\\end{equation*}\n",
    "The ML estimate is the minimizer of the negative log likelihood function with $J(x,y|\\theta)$. Where it is known that the negative likelihood cost function is:\n",
    "\\begin{equation*}\n",
    "    J(\\theta) = arg\\min_{\\theta}(-\\sum log \\hat{p}_{model}(y^{(i)}|x^{(i)},\\theta))\n",
    "\\end{equation*}\n",
    "As well as given Gaussian distribution model parameters with mean and variance. The model of probability model of $\\hat{p}_{model}~\\mathcal{N}\\{y;f(x,\\theta),\\sigma I\\}$ since the output, $p(y_i;\\theta)$ follows normally distribution. Here, the mean is $f(x_i;\\theta)$.\n",
    "Hence, it follows that the estimator of w and b satisfy following:\n",
    "\\begin{equation*}\n",
    "    (\\hat{\\theta}) = arg\\min_{\\theta}(-\\frac{1}{m}\\sum^{m-1}_{i=0}log \\hat{P}_{model(y^i|x^i,\\theta)}) =  arg\\min_{\\theta}(-\\frac{1}{m}\\sum^{m-1}_{i=0}log\\: e^{-\\frac{1}{2\\sigma^2}[y^i-f(x^i;\\theta)][y^i-f(x^i;\\theta)]^T})\n",
    "\\end{equation*}\n",
    "Where the $\\theta $ equals to $(w,b)$.\n",
    "Furthermore, it can develop the probability model which is satisfied as Gaussian normal distribution with the given variance as 1, so the equation can be derived into:\n",
    "\\begin{equation*}\n",
    "    m\\:ln\\sqrt{2\\pi}+\\sum^{m-1}_{i=0}\\frac{((y_i-f(x_i;\\theta))^2}{2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 \n",
    "Get expression for w and b.\n",
    "The expression for the w and b from above can be derived by minimum MSE cost criterion.\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial J}{\\partial w}=\\sum_{0}^{m-1}\\left\\{\\left[y_{i}-\\left(w^{\\top} x_{i}+b\\right)\\right] \\cdot(- x_{i})\\right\\}=0\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial J}{\\partial b}=\\sum_{0}^{m-1}\\left\\{\\left[y_{i}-\\left(w^{\\top} x_{i}+b\\right)\\right] \\cdot (-1)\\right)=0\n",
    "\\end{equation*}\n",
    "From the equations above, the weight w and b thus can be derived as following:\n",
    "\\begin{equation*}\n",
    "    \\sum y_{i}=\\sum w^T x_{i}+m b\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "    b= \\sum\\left(y_{i}-w^{\\top} x_{i}\\right)\n",
    "\\end{equation*}\n",
    "With cost function:\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial J(\\theta)}{\\partial \\theta}= \\sum \\{ [y_i - f(x;\\theta)]\\cdot f(x;\\theta)\\}=0\n",
    "\\end{equation*}\\\\\n",
    "Where actucally, the $ (y-f(x_i;\\theta))^2 = (y-\\theta^Tx)\\cdot (y-\\theta^Tx)^T$, hence the $\\theta$ here is the $\\theta = (XX^T)^{-1}Xy^T$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 and Q4\n",
    "## Q3\n",
    "Calculate the b and w. \n",
    "So in the Q3, the given point is well described.\n",
    "\n",
    "## Q4\n",
    "And result showing that the given parameters are not be capable of reverting back to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  1.  1.  1. ]\n",
      " [0.  0.1 1.  1. ]\n",
      " [0.  1.  0.2 1. ]]\n",
      "(4, 1)\n",
      "the resulst of b, w : [[-1.87350135e-16  1.00000000e-01  4.00000000e-01]]\n",
      "False\n",
      "[[-0.05022928]\n",
      " [ 0.10107389]\n",
      " [ 0.41070998]]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def is_invertible(a):\n",
    "    return a.shape[0] == a.shape[1] and np.linalg.matrix_rank(a) == a.shape[0]\n",
    "\n",
    "X = np.array([[1,0,0],[1,0.1,1],[1,1,0.2],[1,1,1]]).T\n",
    "Y = np.array([0, 0.41, 0.18, 0.5]).reshape(1,4)\n",
    "print(X)\n",
    "print(np.shape(Y.T))\n",
    "t = np.linalg.inv(X@X.T)@X@Y.T # w\n",
    "print('the resulst of b, w :',t.T)\n",
    "\n",
    "print(is_invertible(t))\n",
    "print()\n",
    "\n",
    "Q4= np.array([-0.0416,0.3610,0.1222,0.4733]).reshape(1,4)\n",
    "q4 = np.linalg.inv(X@X.T)@X@Q4.T \n",
    "print(q4)\n",
    "print(is_invertible(q4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5\n",
    "Derive the optimal ngative log-likelihood cost criterion for a diagonal covariance matrix with $\\sigma_0,... \\sigma_N...$ on the diagnoal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "J(\\theta) &=-\\sum_{i=0}^{m-1}\\left[\\ln \\frac{1}{\\sqrt{2 \\pi} \\cdot \\sigma} e^{-\\frac{(y-\\mu)^{2}}{2 \\sigma}}\\right]=m \\ln \\sqrt{2 \\pi} \\cdot \\sigma+\\sum_{i=0}^{m-1} \\frac{(y-\\mu)^{2}}{2 \\sigma^{2}} \\\\\n",
    "&\\left.=m \\ln \\sqrt{2 \\pi}+\\sum_{i=0}^{m-1} \\frac{1}{2}\\left(\\frac{\\left[y_{1}-f\\left(x_{i} ; \\theta)\\right]^{2}\\right.}{\\sigma}\\right)\\right\n",
    "\\end{aligned}\n",
    "Where the covariance can be derived as following:\n",
    "\\begin{equation*}\n",
    "    \\left[\\begin{array}{llll}\n",
    "\\sigma_{1} & & \\\\\n",
    "& \\sigma_{2} & \\\\\n",
    "& & \\sigma_{3} & \\\\\n",
    "& & \\sigma_{N}\n",
    "\\end{array}\\right] \\Rightarrow\\left[\\begin{array}{lllll}\n",
    "\\sigma_{1} & \\sigma_{2} & \\sigma_{3} & \\cdots & \\sigma_{N}\n",
    "\\end{array}\\right] \\cdot\\left[\\begin{array}{c}\n",
    "I\n",
    "\\end{array}\\right]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6\n",
    "## Learning a new funciton, the XOR.\n",
    "### Optimal parameters with XOR of rule.\n",
    "Based on the four possible inputs and outputs of the XOR function.\n",
    "Assume that given input of x data is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]\n",
      " [0 1 1 0]\n",
      " [0 1 0 1]]\n",
      "[[0.0675]\n",
      " [0.045 ]\n",
      " [0.365 ]]\n",
      "[[0.045 0.365]]\n",
      "[[0 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 1]]\n",
      "[[0.0675 0.4775 0.1125 0.4325]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1,0,0],[1,1,1],[1,1,0],[1,0,1]]).T\n",
    "x = np.array([[0,0],[1,1],[1,0],[0,1]])\n",
    "print(X)\n",
    "Y = np.array([0, 0.41, 0.18, 0.5]).reshape(1,4)\n",
    "q6= np.linalg.inv(X@X.T)@X@Y.T # w\n",
    "print(q6)\n",
    "\n",
    "w_1 = np.array([q6[1],q6[2]])\n",
    "print(w_1.T)\n",
    "print(x)\n",
    "print(w_1.T@x.T+q6[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8\n",
    "when $x>> 0$.\\\n",
    "## ReLU\n",
    "From the gradient of the ReLU, the value is either to 1 when x is over 0 or to 0 when x is less than 0.\n",
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9\n",
    "In this turn, it allows you to model a response variable that varies non-linearly with its explanatory variables. In order to map the correlation from the given iniput, however, singals that are transer from linear layer to another linear layer will just give you linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Input from slide\n",
    "# Given\n",
    "W1 = np.array([[1,1],[1,1]])\n",
    "b = np.array([[0],[-1]])\n",
    "b2 = 0\n",
    "W2 = np.array([[1],[-2]])\n",
    "X = np.array([[0,0,1,1],[0,1,0,1]])\n",
    "Y = np.array([0,1,1,0]).reshape(1,4)\n",
    "print(Y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a25b2d42e0fe57251b6ece2b75d30429fe26895efea1faac6949f166a7477be7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
