{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "Q1: Derive the negative log-likelihood cost function J(x, y; Î¸)\n",
    "Consider the following regression network:\n",
    "\\begin{equation*}\n",
    "    f(x;w,b) = w^Tx+b\n",
    "\\end{equation*}\n",
    "And the model has a Gaussian distribution of prediction errors and with a unity covariance matrix $p(y_i;\\theta)$, whereas the mean is $f(x_i;\\theta)$ and variance is one.\\newline\n",
    "It is known that the negative log-likelihood cost function is:\n",
    "\\begin{equation*}\n",
    "    J(\\theta) = -log\\Pi^{m-1}_{i=0}\\hat{P}_{model}(y^{(i)}|x^{(i)},\\theta)\n",
    "\\end{equation*}\n",
    "The ML estimate is the minimizer of the negative log likelihood function with $J(x,y|\\theta)$. Where it is known that the negative likelihood cost function is:\n",
    "\\begin{equation*}\n",
    "    J(\\theta) = arg\\min_{\\theta}(-\\sum log \\hat{p}_{model}(y^{(i)}|x^{(i)},\\theta))\n",
    "\\end{equation*}\n",
    "As well as given Gaussian distribution model parameters with mean and variance. The model of probability model of $\\hat{p}_{model}~\\mathcal{N}\\{y;f(x,\\theta),\\sigma I\\}$ since the output, $p(y_i;\\theta)$ follows normally distribution. Here, the mean is $f(x_i;\\theta)$.\n",
    "Hence, it follows that the estimator of w and b satisfy following:\n",
    "\\begin{equation*}\n",
    "    (\\hat{\\theta}) = arg\\min_{\\theta}(-\\frac{1}{m}\\sum^{m-1}_{i=0}log \\hat{P}_{model(y^i|x^i,\\theta)}) =  arg\\min_{\\theta}(-\\frac{1}{m}\\sum^{m-1}_{i=0}log\\: e^{-\\frac{1}{2\\sigma^2}[y^i-f(x^i;\\theta)][y^i-f(x^i;\\theta)]^T})\n",
    "\\end{equation*}\n",
    "Where the $\\theta $ equals to $(w,b)$.\n",
    "Furthermore, it can develop the probability model which is satisfied as Gaussian normal distribution with the given variance as 1, so the equation can be derived into:\n",
    "\\begin{equation*}\n",
    "    m\\:ln\\sqrt{2\\pi}+\\sum^{m-1}_{i=0}\\frac{((y_i-f(x_i;\\theta))^2}{2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 \n",
    "Get expression for w and b.\n",
    "The expression for the w and b from above can be derived by minimum MSE cost criterion.\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial J}{\\partial w}=\\sum_{0}^{m-1}\\left\\{\\left[y_{i}-\\left(w^{\\top} x_{i}+b\\right)\\right] \\cdot(- x_{i})\\right\\}=0\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial J}{\\partial b}=\\sum_{0}^{m-1}\\left\\{\\left[y_{i}-\\left(w^{\\top} x_{i}+b\\right)\\right] \\cdot (-1)\\right)=0\n",
    "\\end{equation*}\n",
    "From the equations above, the weight w and b thus can be derived as following:\n",
    "\\begin{equation*}\n",
    "    \\sum y_{i}=\\sum w^T x_{i}+m b\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "    b= \\sum\\left(y_{i}-w^{\\top} x_{i}\\right)\n",
    "\\end{equation*}\n",
    "With cost function:\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial J(\\theta)}{\\partial \\theta}= \\sum \\{ [y_i - f(x;\\theta)]\\cdot f(x;\\theta)\\}=0\n",
    "\\end{equation*}\\\\\n",
    "Where actucally, the $ (y-f(x_i;\\theta))^2 = (y-\\theta^Tx)\\cdot (y-\\theta^Tx)^T$, hence the $\\theta$ here is the $\\theta = (XX^T)^{-1}Xy^T$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 and Q4\n",
    "Calculate the b and w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  1.  1.  1. ]\n",
      " [0.  0.1 1.  1. ]\n",
      " [0.  1.  0.2 1. ]]\n",
      "(4, 1)\n",
      "[[-1.87350135e-16]\n",
      " [ 1.00000000e-01]\n",
      " [ 4.00000000e-01]]\n",
      "[[-0.05022928]\n",
      " [ 0.10107389]\n",
      " [ 0.41070998]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1,0,0],[1,0.1,1],[1,1,0.2],[1,1,1]]).T\n",
    "Y = np.array([0, 0.41, 0.18, 0.5]).reshape(1,4)\n",
    "print(X)\n",
    "print(np.shape(Y.T))\n",
    "t = np.linalg.inv(X@X.T)@X@Y.T\n",
    "print(t)\n",
    "Q4= np.array([-0.0416,0.3610,0.1222,0.4733]).reshape(1,4)\n",
    "\n",
    "q4 = np.linalg.inv(X@X.T)@X@Q4.T\n",
    "print(q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5\n",
    "Derive the optimal ngative log-likelihood cost criterion for a diagonal covariance matrix with $\\sigma_0,... \\sigma_N...$ on the diagnoal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a25b2d42e0fe57251b6ece2b75d30429fe26895efea1faac6949f166a7477be7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
